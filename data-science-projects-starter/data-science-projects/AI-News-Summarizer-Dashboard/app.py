# AI News Summarizer Dashboard
# Streamlit app for summarizing news/content from URLs, pasted text, or uploaded files.

import io
import re
from typing import Optional

import streamlit as st

# Optional dependencies handled gracefully
try:
    import trafilatura
except Exception:
    trafilatura = None

try:
    from sumy.parsers.plaintext import PlaintextParser
    from sumy.nlp.tokenizers import Tokenizer
    from sumy.summarizers.lex_rank import LexRankSummarizer
except Exception:
    PlaintextParser = Tokenizer = LexRankSummarizer = None

# Transformers (huggingface) for abstractive summarization
from transformers import pipeline

APP_TITLE = "ðŸ“° AI News Summarizer Dashboard"
APP_DESC = (
    "Paste a URL, text, or upload a file to generate a clean summary. "
    "Supports HuggingFace transformers (abstractive) with a classical extractive fallback."
)

st.set_page_config(page_title="AI News Summarizer", page_icon="ðŸ“°", layout="centered")

# -----------------------------
# Utilities
# -----------------------------

def clean_text(text: str) -> str:
    if not text:
        return ""
    # Remove excessive whitespace
    text = re.sub(r"\s+", " ", text).strip()
    # Keep it reasonably sized for models
    return text

def read_uploaded_file(file) -> str:
    name = file.name.lower()
    data = file.read()
    if name.endswith(".txt") or name.endswith(".md"):
        return data.decode("utf-8", errors="ignore")
    if name.endswith(".pdf"):
        try:
            from pdfminer.high_level import extract_text
            file.seek(0)
            return extract_text(io.BytesIO(data))
        except Exception as e:
            st.warning(f"PDF è§£æžå¤±æ•—ï¼š{e}")
            return ""
    if name.endswith(".docx"):
        try:
            import docx
            doc = docx.Document(io.BytesIO(data))
            return "\n".join(p.text for p in doc.paragraphs)
        except Exception as e:
            st.warning(f"DOCX è§£æžå¤±æ•—ï¼š{e}")
            return ""
    # Fallback â€“ try decoding as text
    try:
        return data.decode("utf-8", errors="ignore")
    except Exception:
        return ""

def extract_from_url(url: str) -> str:
    if trafilatura is None:
        st.info("æœªå®‰è£ trafilaturaï¼Œæ”¹ç”¨è²¼ä¸Šæ–‡å­—æˆ–ä¸Šå‚³æª”æ¡ˆã€‚")
        return ""
    try:
        downloaded = trafilatura.fetch_url(url)
        return trafilatura.extract(downloaded) or ""
    except Exception as e:
        st.warning(f"æ“·å–å¤±æ•—ï¼š{e}")
        return ""

def sumy_lexrank_summary(text: str, sentences: int = 5) -> str:
    if not (PlaintextParser and Tokenizer and LexRankSummarizer):
        return ""
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = LexRankSummarizer()
        sents = summarizer(parser.document, sentences)
        return " ".join(str(s) for s in sents)
    except Exception:
        return ""

@st.cache_resource(show_spinner=False)
def load_summarizer(model_name: str):
    # device_map="auto" lets transformers choose GPU if available; falls back to CPU.
    return pipeline(
        "summarization",
        model=model_name,
        device_map="auto",
    )

def hf_summarize(text: str, model_name: str, max_words: int = 220) -> str:
    # Transformers use tokens; set a reasonable token budget.
    # Rough token estimate ~ 1.3x words
    max_tokens = int(max_words * 1.3)
    min_tokens = max(30, int(max_tokens * 0.4))
    summarizer = load_summarizer(model_name)
    # Many models have max input token limits; chunk if needed.
    chunks = chunk_text_for_model(text, max_chunk_chars=2500)
    outputs = []
    for chunk in chunks:
        out = summarizer(chunk, max_length=max_tokens, min_length=min_tokens, do_sample=False, truncation=True)
        outputs.append(out[0]["summary_text"])
    return " ".join(outputs)

def chunk_text_for_model(text: str, max_chunk_chars: int = 2500):
    text = text.strip()
    if len(text) <= max_chunk_chars:
        return [text]
    # Try to split on sentence boundaries
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ.!?])\s+', text)
    current = ""
    chunks = []
    for s in sentences:
        if len(current) + len(s) + 1 <= max_chunk_chars:
            current += (" " if current else "") + s
        else:
            if current:
                chunks.append(current)
            current = s
    if current:
        chunks.append(current)
    return chunks

# -----------------------------
# UI
# -----------------------------

st.title(APP_TITLE)
st.caption(APP_DESC)

with st.sidebar:
    st.subheader("è¨­å®š / Settings")
    model_choice = st.selectbox(
        "HuggingFace æ¨¡åž‹ (Abstractive)",
        options=[
            "facebook/bart-large-cnn",
            "philschmid/bart-large-cnn-samsum",
            "sshleifer/distilbart-cnn-12-6",
            "google/pegasus-xsum",
            "t5-small",
        ],
        index=0,
        help="é¸æ“‡æ‘˜è¦æ¨¡åž‹ã€‚è‹¥é‡åˆ°éŒ¯èª¤ï¼Œè«‹æ›ä¸€å€‹è¼•é‡æ¨¡åž‹ã€‚",
    )
    target_words = st.slider("æ‘˜è¦é•·åº¦ (ä¼°è¨ˆå­—æ•¸)", 80, 400, 180, 10)

tab1, tab2, tab3 = st.tabs(["ðŸ”— å¾žç¶²å€æ“·å–", "ðŸ“ ç›´æŽ¥è²¼ä¸Šæ–‡å­—", "ðŸ“ ä¸Šå‚³æª”æ¡ˆ"])

with tab1:
    url = st.text_input("è²¼ä¸Šæ–°èžæˆ–æ–‡ç« ç¶²å€ (URL)", value="", placeholder="https://example.com/news")
    if st.button("æ“·å–ä¸¦æ‘˜è¦", type="primary", use_container_width=True, key="summ_from_url"):
        if not url:
            st.warning("è«‹è¼¸å…¥ç¶²å€ã€‚")
        else:
            with st.spinner("æ“·å–èˆ‡æ‘˜è¦ä¸­â€¦"):
                content = extract_from_url(url)
                content = clean_text(content)
                if not content or len(content) < 120:
                    st.error("æ“·å–å…§å®¹éŽçŸ­æˆ–å¤±æ•—ï¼Œè«‹æ”¹ç”¨è²¼ä¸Šæ–‡å­—æˆ–ä¸Šå‚³æª”æ¡ˆã€‚")
                else:
                    try:
                        summary = hf_summarize(content, model_choice, max_words=target_words)
                    except Exception as e:
                        st.warning(f"HuggingFace æ‘˜è¦å¤±æ•—ï¼Œæ”¹ç”¨å‚³çµ±æ‘˜è¦ã€‚éŒ¯èª¤ï¼š{e}")
                        summary = sumy_lexrank_summary(content) or "ï¼ˆå‚™æ´æ‘˜è¦ä¹Ÿå¤±æ•—ï¼Œè«‹å˜—è©¦ä¸åŒè¼¸å…¥æˆ–æ¨¡åž‹ã€‚ï¼‰"
                    st.success("å®Œæˆï¼")
                    st.subheader("æ‘˜è¦ / Summary")
                    st.write(summary)
                    with st.expander("åŽŸå§‹æ“·å–å…§å®¹"):
                        st.write(content[:5000])

with tab2:
    user_text = st.text_area("è²¼ä¸Šä½ è¦æ‘˜è¦çš„å…§å®¹", height=220, placeholder="åœ¨æ­¤è²¼ä¸Šæ–‡ç« æˆ–ç­†è¨˜â€¦")
    if st.button("ç”¢ç”Ÿæ‘˜è¦", type="primary", use_container_width=True, key="summ_from_text"):
        text = clean_text(user_text)
        if len(text) < 50:
            st.warning("å…§å®¹å¤ªçŸ­ï¼Œè«‹æä¾›æ›´å®Œæ•´çš„æ–‡å­—ã€‚")
        else:
            with st.spinner("ç”¢ç”Ÿæ‘˜è¦ä¸­â€¦"):
                try:
                    summary = hf_summarize(text, model_choice, max_words=target_words)
                except Exception as e:
                    st.warning(f"HuggingFace æ‘˜è¦å¤±æ•—ï¼Œæ”¹ç”¨å‚³çµ±æ‘˜è¦ã€‚éŒ¯èª¤ï¼š{e}")
                    summary = sumy_lexrank_summary(text) or "ï¼ˆå‚™æ´æ‘˜è¦ä¹Ÿå¤±æ•—ï¼Œè«‹å˜—è©¦ä¸åŒè¼¸å…¥æˆ–æ¨¡åž‹ã€‚ï¼‰"
            st.subheader("æ‘˜è¦ / Summary")
            st.write(summary)

with tab3:
    upload = st.file_uploader("ä¸Šå‚³ .txt / .md / .pdf / .docx", type=["txt", "md", "pdf", "docx"])
    if upload and st.button("è®€å–ä¸¦æ‘˜è¦", type="primary", use_container_width=True, key="summ_from_file"):
        with st.spinner("è®€å–èˆ‡æ‘˜è¦ä¸­â€¦"):
            content = read_uploaded_file(upload)
            content = clean_text(content)
            if len(content) < 80:
                st.warning("æª”æ¡ˆå…§å®¹éŽçŸ­æˆ–è§£æžå¤±æ•—ã€‚")
            else:
                try:
                    summary = hf_summarize(content, model_choice, max_words=target_words)
                except Exception as e:
                    st.warning(f"HuggingFace æ‘˜è¦å¤±æ•—ï¼Œæ”¹ç”¨å‚³çµ±æ‘˜è¦ã€‚éŒ¯èª¤ï¼š{e}")
                    summary = sumy_lexrank_summary(content) or "ï¼ˆå‚™æ´æ‘˜è¦ä¹Ÿå¤±æ•—ï¼Œè«‹å˜—è©¦ä¸åŒè¼¸å…¥æˆ–æ¨¡åž‹ã€‚ï¼‰"
                st.subheader("æ‘˜è¦ / Summary")
                st.write(summary)

st.markdown("---")
st.caption("Tip: è‹¥é‡åˆ°æ¨¡åž‹ä¸‹è¼‰éŽæ…¢æˆ–å¤±æ•—ï¼Œå¯æ”¹é¸æ“‡è¼ƒå°çš„æ¨¡åž‹ (å¦‚ t5-small)ï¼Œæˆ–åœ¨æœ¬åœ°å…ˆè¡Œä¸‹è¼‰æ¨¡åž‹ã€‚")
