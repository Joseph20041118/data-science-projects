{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI News Summarizer â€“ Demo Notebook\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "- Use Hugging Face transformers to summarize text.\n",
        "- Extract article text from a URL with `trafilatura`.\n",
        "- Fall back to a classical extractive method (`sumy`) if needed.\n",
        "\n",
        "Run the cells below. You can also open and modify the functions for your own pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install requirements in a fresh environment\n",
        "# !pip install -r ../requirements.txt\n",
        "import re, io\n",
        "from typing import List\n",
        "\n",
        "try:\n",
        "    import trafilatura\n",
        "except Exception:\n",
        "    trafilatura = None\n",
        "\n",
        "try:\n",
        "    from sumy.parsers.plaintext import PlaintextParser\n",
        "    from sumy.nlp.tokenizers import Tokenizer\n",
        "    from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "except Exception:\n",
        "    PlaintextParser = Tokenizer = LexRankSummarizer = None\n",
        "\n",
        "from transformers import pipeline\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r\"\\s+\", \" \", text or \"\").strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text_for_model(text: str, max_chunk_chars: int = 2500) -> List[str]:\n",
        "    text = text.strip()\n",
        "    if len(text) <= max_chunk_chars:\n",
        "        return [text]\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    current, chunks = \"\", []\n",
        "    for s in sentences:\n",
        "        if len(current) + len(s) + 1 <= max_chunk_chars:\n",
        "            current += (\" \" if current else \"\") + s\n",
        "        else:\n",
        "            if current:\n",
        "                chunks.append(current)\n",
        "            current = s\n",
        "    if current:\n",
        "        chunks.append(current)\n",
        "    return chunks\n",
        "\n",
        "def load_summarizer(model_name: str = \"sshleifer/distilbart-cnn-12-6\"):\n",
        "    return pipeline(\"summarization\", model=model_name, device_map=\"auto\")\n",
        "\n",
        "def hf_summarize(text: str, model_name: str = \"sshleifer/distilbart-cnn-12-6\", max_words: int = 220) -> str:\n",
        "    max_tokens = int(max_words * 1.3)\n",
        "    min_tokens = max(30, int(max_tokens * 0.4))\n",
        "    summarizer = load_summarizer(model_name)\n",
        "    chunks = chunk_text_for_model(text)\n",
        "    outputs = []\n",
        "    for ch in chunks:\n",
        "        out = summarizer(ch, max_length=max_tokens, min_length=min_tokens, do_sample=False, truncation=True)\n",
        "        outputs.append(out[0][\"summary_text\"])\n",
        "    return \" \".join(outputs)\n",
        "\n",
        "def sumy_lexrank_summary(text: str, sentences: int = 5) -> str:\n",
        "    if not (PlaintextParser and Tokenizer and LexRankSummarizer):\n",
        "        return \"\"\n",
        "    try:\n",
        "        parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "        summarizer = LexRankSummarizer()\n",
        "        sents = summarizer(parser.document, sentences)\n",
        "        return \" \".join(str(s) for s in sents)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "print('Utility functions ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example A: summarize a URL (if trafilatura is available)\n",
        "url = \"https://example.com/news\"\n",
        "if trafilatura is None:\n",
        "    print(\"trafilatura not installed; skipping URL extraction.\")\n",
        "else:\n",
        "    try:\n",
        "        downloaded = trafilatura.fetch_url(url)\n",
        "        article = trafilatura.extract(downloaded) or \"\"\n",
        "        article = clean_text(article)\n",
        "        if len(article) > 100:\n",
        "            print('Extracted chars:', len(article))\n",
        "            print('\\nHF Summary:\\n', hf_summarize(article))\n",
        "        else:\n",
        "            print('Extraction yielded very short content; try another URL.')\n",
        "    except Exception as e:\n",
        "        print('Extraction error:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example B: summarize pasted text\n",
        "sample_text = (\n",
        "    \"OpenAI's recent advancements in natural language processing have accelerated the adoption of AI across industries. \"\n",
        "    \"From customer support automation to content generation, organizations are increasingly relying on transformer-based models. \"\n",
        "    \"However, challenges remain, including responsible deployment, evaluation, and governance.\" \n",
        "    \" This notebook demonstrates a simple summarization pipeline using transformers with an extractive fallback.\"\n",
        ")\n",
        "sample_text = clean_text(sample_text)\n",
        "print('Original chars:', len(sample_text))\n",
        "print('\\nHF Summary:\\n', hf_summarize(sample_text))\n",
        "fallback = sumy_lexrank_summary(sample_text)\n",
        "if fallback:\n",
        "    print('\\nLexRank Fallback Summary:\\n', fallback)\n",
        "else:\n",
        "    print('\\nLexRank not available in this environment.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}